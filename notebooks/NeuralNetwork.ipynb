{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coCWqLxU2lMo",
   "metadata": {
    "id": "coCWqLxU2lMo"
   },
   "source": [
    "## Подключаем необходимые модули"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "id": "y7AlJOfC29Q0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T12:49:11.269100Z",
     "start_time": "2023-10-24T12:49:09.520270Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in c:\\python311\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in c:\\python311\\lib\\site-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in c:\\python311\\lib\\site-packages (from tensorflow_addons) (23.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install  tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "iCaLPVesuaSm",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T12:47:22.653885Z",
     "start_time": "2023-10-24T12:47:22.651120Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/lyftzeigen/SemanticSegmentationLesson.git"
   ]
=======
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !pip install tensorflow-addons\n",
    "!python --version"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "y7AlJOfC29Q0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !git clone https://github.com/lyftzeigen/SemanticSegmentationLesson.git"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "475ce30367702cb8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "SYSTEM_VERSION_COMPAT=0 pip install tensorflow-macos tensorflow-metal\n",
    "pip install chardet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c76f244eb342d482"
>>>>>>> 423d934863baf4e74d3d104450ff068781864bd2
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f0321d-9fcb-4e27-a8d9-e9986ae56179",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-24T12:49:14.145127Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mДля выполнения ячеек с \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\" требуется пакет ipykernel.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/admin/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "from  matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from skimage import measure\n",
    "from skimage.io import imread, imsave, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage.filters import gaussian\n",
    "from skimage.morphology import dilation, disk\n",
    "from skimage.draw import polygon, polygon_perimeter\n",
    "\n",
    "print(f'Tensorflow version {tf.__version__}')\n",
    "print(f'GPU is {\"ON\" if tf.config.list_physical_devices(\"GPU\") else \"OFF\" }')"
<<<<<<< HEAD
   ]
=======
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8f0321d-9fcb-4e27-a8d9-e9986ae56179"
>>>>>>> 423d934863baf4e74d3d104450ff068781864bd2
  },
  {
   "cell_type": "markdown",
   "id": "e9e5c5b4-6daa-4dee-8be6-dd67343bf3d3",
   "metadata": {
    "id": "e9e5c5b4-6daa-4dee-8be6-dd67343bf3d3"
   },
   "source": [
    "## Подготовим набор данных для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be98ae-bc6e-4d49-b205-db5130879caa",
   "metadata": {
    "id": "82be98ae-bc6e-4d49-b205-db5130879caa"
   },
   "outputs": [],
   "source": [
    "CLASSES = 8\n",
    "\n",
    "COLORS = ['black', 'red', 'lime',\n",
    "          'blue', 'orange', 'pink',\n",
    "          'cyan', 'magenta']\n",
    "\n",
    "SAMPLE_SIZE = (256, 256)\n",
    "\n",
    "OUTPUT_SIZE = (1080, 1920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500caf45-f2b9-48af-8f91-7d31d44ec266",
   "metadata": {
    "id": "500caf45-f2b9-48af-8f91-7d31d44ec266"
   },
   "outputs": [],
   "source": [
    "def load_images(image, mask):\n",
    "    image = tf.io.read_file(image)\n",
    "    image = tf.io.decode_jpeg(image)\n",
    "    image = tf.image.resize(image, OUTPUT_SIZE)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = image / 255.0\n",
    "    \n",
    "    mask = tf.io.read_file(mask)\n",
    "    mask = tf.io.decode_png(mask)\n",
    "    mask = tf.image.rgb_to_grayscale(mask)\n",
    "    mask = tf.image.resize(mask, OUTPUT_SIZE)\n",
    "    mask = tf.image.convert_image_dtype(mask, tf.float32)\n",
    "    \n",
    "    masks = []\n",
    "    # images = []\n",
    "    \n",
    "    for i in range(CLASSES):\n",
    "        masks.append(tf.where(tf.equal(mask, float(i)), 1.0, 0.0))\n",
    "        # images.append(tf.where(tf.equal(image, float(i)), 1.0, 0.0))\n",
    "\n",
    "    masks = tf.stack(masks, axis=2)\n",
    "    masks = tf.reshape(masks, OUTPUT_SIZE + (CLASSES,))\n",
    "    # images = tf.stack(images, axis=2)\n",
    "    # images = tf.reshape(images, OUTPUT_SIZE + (CLASSES,))\n",
    "    print(image, masks)\n",
    "    return image, masks\n",
    "\n",
    "def augmentate_images(image, masks):   \n",
    "    random_crop = tf.random.uniform((), 0.3, 1)\n",
    "    image = tf.image.central_crop(image, random_crop)\n",
    "    masks = tf.image.central_crop(masks, random_crop)\n",
    "    \n",
    "    random_flip = tf.random.uniform((), 0, 1)    \n",
    "    if random_flip >= 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        masks = tf.image.flip_left_right(masks)\n",
    "    \n",
    "    image = tf.image.resize(image, SAMPLE_SIZE)\n",
    "    masks = tf.image.resize(masks, SAMPLE_SIZE)\n",
    "    \n",
    "    return image, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ac3f3-17b7-4fe5-9622-c7d82fbdaff2",
   "metadata": {
    "id": "1d6ac3f3-17b7-4fe5-9622-c7d82fbdaff2"
   },
   "outputs": [],
   "source": [
    "images = sorted(glob.glob('../data/dataset/images/*.jpg'))[:10]\n",
    "masks = sorted(glob.glob('../data/dataset/masks/*.png'))\n",
    "\n",
    "images_dataset = tf.data.Dataset.from_tensor_slices(images)\n",
    "masks_dataset = tf.data.Dataset.from_tensor_slices(masks)\n",
    "\n",
    "dataset = tf.data.Dataset.zip((images_dataset, masks_dataset))\n",
    "dataset = dataset.map(load_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.repeat(60)\n",
    "dataset = dataset.map(augmentate_images, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd0f38c-d6f6-48f6-842e-d392ecf6a923",
   "metadata": {
    "id": "0cd0f38c-d6f6-48f6-842e-d392ecf6a923"
   },
   "source": [
    "## Посмотрим на содержимое набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe418202-ca78-4f37-a5e0-4a195fb9a8b2",
   "metadata": {
    "id": "fe418202-ca78-4f37-a5e0-4a195fb9a8b2"
   },
   "outputs": [],
   "source": [
    "images_and_masks = list(dataset.take(5))\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 2, ncols = 5, figsize=(15, 5), dpi=125)\n",
    "\n",
    "for i, (image, masks) in enumerate(images_and_masks):\n",
    "    ax[0, i].set_title('Image')\n",
    "    ax[0, i].set_axis_off()\n",
    "    ax[0, i].imshow(image)\n",
    "        \n",
    "    ax[1, i].set_title('Mask')\n",
    "    ax[1, i].set_axis_off()    \n",
    "    ax[1, i].imshow(image/1.5)\n",
    "   \n",
    "    for channel in range(CLASSES):\n",
    "        contours = measure.find_contours(np.array(masks[:,:,channel]))\n",
    "        for contour in contours:\n",
    "            ax[1, i].plot(contour[:, 1], contour[:, 0], linewidth=1, color=COLORS[channel])\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2c4bf-74e0-41de-86c6-e89687ef3ca4",
   "metadata": {
    "id": "a5a2c4bf-74e0-41de-86c6-e89687ef3ca4"
   },
   "source": [
    "## Разделим набор данных на обучающий и проверочный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e3f67-5f19-48f3-9a9f-09660b64d49a",
   "metadata": {
    "id": "cd9e3f67-5f19-48f3-9a9f-09660b64d49a"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.take(26).cache()\n",
    "test_dataset = dataset.skip(26).take(8).cache()\n",
    " \n",
<<<<<<< HEAD
    "train_dataset = train_dataset.batch(16)\n",
    "test_dataset = test_dataset.batch(16)"
   ]
=======
    "train_dataset = train_dataset.batch(2)\n",
    "test_dataset = test_dataset.batch(2)"
   ],
   "id": "cd9e3f67-5f19-48f3-9a9f-09660b64d49a"
>>>>>>> 423d934863baf4e74d3d104450ff068781864bd2
  },
  {
   "cell_type": "markdown",
   "id": "482b3f41-5324-41e1-944d-809ec06ee959",
   "metadata": {
    "id": "482b3f41-5324-41e1-944d-809ec06ee959"
   },
   "source": [
    "## Обозначим основные блоки модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b6dcd3-d04f-4c6c-bca7-7315df18ff4b",
   "metadata": {
    "id": "69b6dcd3-d04f-4c6c-bca7-7315df18ff4b"
   },
   "outputs": [],
   "source": [
    "def input_layer():\n",
    "    return tf.keras.layers.Input(shape=SAMPLE_SIZE + (3,))\n",
    "\n",
    "def downsample_block(filters, size, batch_norm=True):\n",
    "    initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    \n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if batch_norm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "    return result\n",
    "\n",
    "def upsample_block(filters, size, dropout=False):\n",
    "    initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    \n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n",
    "                                        kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    if dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.25))\n",
    "    \n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "    return result\n",
    "\n",
    "def output_layer(size):\n",
    "    initializer = tf.keras.initializers.GlorotNormal()\n",
    "    return tf.keras.layers.Conv2DTranspose(CLASSES, size, strides=2, padding='same',\n",
    "                                           kernel_initializer=initializer, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ea1c64-11ac-485c-bcde-b4059bd74edc",
   "metadata": {
    "id": "a4ea1c64-11ac-485c-bcde-b4059bd74edc"
   },
   "source": [
    "## Построим U-NET подобную архитектуру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2ece8-75f3-46b9-8918-f4034f223b40",
   "metadata": {
    "id": "a4f2ece8-75f3-46b9-8918-f4034f223b40"
   },
   "outputs": [],
   "source": [
    "inp_layer = input_layer()\n",
    "\n",
    "downsample_stack = [\n",
    "    downsample_block(64, 4, batch_norm=False),\n",
    "    downsample_block(128, 4),\n",
    "    downsample_block(256, 4),\n",
    "    downsample_block(512, 4),\n",
    "    downsample_block(512, 4),\n",
    "    downsample_block(512, 4),\n",
    "    downsample_block(512, 4),\n",
    "]\n",
    "\n",
    "upsample_stack = [\n",
    "    upsample_block(512, 4, dropout=True),\n",
    "    upsample_block(512, 4, dropout=True),\n",
    "    upsample_block(512, 4, dropout=True),\n",
    "    upsample_block(256, 4),\n",
    "    upsample_block(128, 4),\n",
    "    upsample_block(64, 4)\n",
    "]\n",
    "\n",
    "out_layer = output_layer(4)\n",
    "\n",
    "# Реализуем skip connections\n",
    "x = inp_layer\n",
    "\n",
    "downsample_skips = []\n",
    "\n",
    "for block in downsample_stack:\n",
    "    x = block(x)\n",
    "    downsample_skips.append(x)\n",
    "    \n",
    "downsample_skips = reversed(downsample_skips[:-1])\n",
    "\n",
    "for up_block, down_block in zip(upsample_stack, downsample_skips):\n",
    "    x = up_block(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, down_block])\n",
    "\n",
    "out_layer = out_layer(x)\n",
    "\n",
    "unet_like = tf.keras.Model(inputs=inp_layer, outputs=out_layer)\n",
    "\n",
    "tf.keras.utils.plot_model(unet_like, show_shapes=True, dpi=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67f732f-120b-49a8-bc7c-304709f12db5",
   "metadata": {
    "id": "b67f732f-120b-49a8-bc7c-304709f12db5"
   },
   "source": [
    "## Определим метрики и функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6aa6d8-f478-4d28-acfb-c217c112c381",
   "metadata": {
    "id": "9a6aa6d8-f478-4d28-acfb-c217c112c381"
   },
   "outputs": [],
   "source": [
    "def dice_mc_metric(a, b):\n",
    "    a = tf.unstack(a, axis=3)\n",
    "    b = tf.unstack(b, axis=3)\n",
    "    \n",
    "    dice_summ = 0\n",
    "    \n",
    "    for i, (aa, bb) in enumerate(zip(a, b)):\n",
    "        numenator = 2 * tf.math.reduce_sum(aa * bb) + 1\n",
    "        denomerator = tf.math.reduce_sum(aa + bb) + 1\n",
    "        dice_summ += numenator / denomerator\n",
    "        \n",
    "    avg_dice = dice_summ / CLASSES\n",
    "    \n",
    "    return avg_dice\n",
    "\n",
    "def dice_mc_loss(a, b):\n",
    "    return 1 - dice_mc_metric(a, b)\n",
    "\n",
    "def dice_bce_mc_loss(a, b):\n",
    "    return 0.3 * dice_mc_loss(a, b) + tf.keras.losses.binary_crossentropy(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b439664c-71f0-49ac-a462-4d60cc5ec77c",
   "metadata": {
    "id": "b439664c-71f0-49ac-a462-4d60cc5ec77c"
   },
   "source": [
    "## Компилируем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa219c12-c5af-4065-8660-fb8d6aa7d2a1",
   "metadata": {
    "id": "fa219c12-c5af-4065-8660-fb8d6aa7d2a1"
   },
   "outputs": [],
   "source": [
    "unet_like.compile(optimizer='adam', loss=[dice_bce_mc_loss], metrics=[dice_mc_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bfd329-31b4-4200-8282-1cb066d52b83",
   "metadata": {
    "id": "75bfd329-31b4-4200-8282-1cb066d52b83"
   },
   "source": [
    "## Обучаем нейронную сеть и сохраняем результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d6d6e-9c85-49e0-8601-37d90d7eb7cb",
   "metadata": {
    "id": "215d6d6e-9c85-49e0-8601-37d90d7eb7cb"
   },
   "outputs": [],
   "source": [
    "history_dice = unet_like.fit(train_dataset, validation_data=test_dataset, epochs=2, initial_epoch=0)\n",
    "\n",
<<<<<<< HEAD
    "unet_like.save_weights('data/networks/unet_like')"
   ]
=======
    "unet_like.save_weights('../data/dataset/networks/unet_like')"
   ],
   "id": "215d6d6e-9c85-49e0-8601-37d90d7eb7cb"
>>>>>>> 423d934863baf4e74d3d104450ff068781864bd2
  },
  {
   "cell_type": "markdown",
   "id": "311e395e-5465-4507-a75c-7ea95dc19e69",
   "metadata": {
    "id": "311e395e-5465-4507-a75c-7ea95dc19e69"
   },
   "source": [
    "## Загрузим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f70032-8e6d-4b3f-b078-f227bf90ab0c",
   "metadata": {
    "id": "67f70032-8e6d-4b3f-b078-f227bf90ab0c"
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "unet_like.load_weights('data/networks/unet_like')"
   ]
=======
    "unet_like.load_weights('../data/dataset/networks/unet_like')"
   ],
   "id": "67f70032-8e6d-4b3f-b078-f227bf90ab0c"
>>>>>>> 423d934863baf4e74d3d104450ff068781864bd2
  },
  {
   "cell_type": "markdown",
   "id": "e978c692-4136-419b-9365-e5fbf98bbf50",
   "metadata": {
    "id": "e978c692-4136-419b-9365-e5fbf98bbf50"
   },
   "source": [
    "## Проверим работу сети на всех кадрах из видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b67499-d8a3-42a1-a990-b2b7184ad75c",
   "metadata": {
    "id": "59b67499-d8a3-42a1-a990-b2b7184ad75c"
   },
   "outputs": [],
   "source": [
    "rgb_colors = [\n",
    "    (0,   0,   0),\n",
    "    (255, 0,   0),\n",
    "    (0,   255, 0),\n",
    "    (0,   0,   255),\n",
    "    (255, 165, 0),\n",
    "    (255, 192, 203),\n",
    "    (0,   255, 255),\n",
    "    (255, 0,   255)\n",
    "]\n",
    "\n",
    "frames = sorted(glob.glob('../data/videos/original_video/*.jpg'))\n",
    "\n",
    "for filename in frames:\n",
    "    frame = imread(filename)\n",
    "    sample = resize(frame, SAMPLE_SIZE)\n",
    "    \n",
    "    predict = unet_like.predict(sample.reshape((1,) +  SAMPLE_SIZE + (3,)))\n",
    "    print(predict)\n",
    "    predict = predict.reshape(SAMPLE_SIZE + (CLASSES,))\n",
    "        \n",
    "    scale = frame.shape[0] / SAMPLE_SIZE[0], frame.shape[1] / SAMPLE_SIZE[1]\n",
    "    \n",
    "    frame = (frame / 1.5).astype(np.uint8)\n",
    "    \n",
    "    print(len(predict[1,1,:]))\n",
    "    for channel in range(1, CLASSES): \n",
    "        contour_overlay = np.zeros((frame.shape[0], frame.shape[1]))\n",
    "        contours = measure.find_contours(np.array(predict[:,:,channel]))\n",
    "        \n",
    "        try:\n",
    "            for contour in contours:\n",
    "                rr, cc = polygon_perimeter(contour[:, 0] * scale[0],\n",
    "                                           contour[:, 1] * scale[1],\n",
    "                                           shape=contour_overlay.shape)\n",
    "                \n",
    "                contour_overlay[rr, cc] = 1        \n",
    "            \n",
    "            contour_overlay = dilation(contour_overlay, disk(1))\n",
    "            frame[contour_overlay == 1] = rgb_colors[channel]\n",
    "        except:\n",
    "            pass\n",
    "\n",
<<<<<<< HEAD
    "    imsave(f'data/videos/processed/{os.path.basename(filename)}', frame)"
   ]
=======
    "    imsave(f'../data/videos/processed/{os.path.basename(filename)}', frame)"
   ],
   "id": "59b67499-d8a3-42a1-a990-b2b7184ad75c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7552e6f6194c932f"
>>>>>>> 423d934863baf4e74d3d104450ff068781864bd2
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NeuralNetwork.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
